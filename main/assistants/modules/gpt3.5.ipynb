{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "print(os.environ['OPENAI_API_KEY'])\n",
    "# openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떻게 짤까\n",
    "\n",
    "음....\n",
    "\n",
    "번역은 잘 되니깐 그걸로\n",
    "\n",
    "번역기를 두 번돌리도록 해야하나\n",
    "\n",
    "솔라 써볼까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "conversation = ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'TavilySearchResults'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     27\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 문서를 로드하고 분할합니다.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# split_docs = loader.load_and_split(text_splitter)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m### 1-3. tools 리스트에 도구 목록을 추가합니다 ###\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# tools 리스트에 search와 retriever_tool을 추가합니다.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[43mload_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m########## 2. LLM 을 정의합니다 ##########\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# LLM 모델을 생성합니다.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbllossom-8b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\langchain_community\\agent_toolkits\\load_tools.py:673\u001b[0m, in \u001b[0;36mload_tools\u001b[1;34m(tool_names, llm, callbacks, allow_dangerous_tools, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _handle_callbacks(\n\u001b[0;32m    670\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m), callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m    671\u001b[0m )\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m tool_names:\n\u001b[1;32m--> 673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDANGEROUS_TOOLS\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_dangerous_tools:\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    675\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a dangerous tool. You cannot use it without opting in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    676\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby setting allow_dangerous_tools to True. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    688\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'TavilySearchResults'"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 import\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "########## 1. 도구를 정의합니다 ##########\n",
    "\n",
    "### 1-1. Search 도구 ###\n",
    "# TavilySearchResults 클래스의 인스턴스를 생성합니다\n",
    "# k=5은 검색 결과를 5개까지 가져오겠다는 의미입니다\n",
    "search = TavilySearchResults(k=5)\n",
    "\n",
    "### 1-2. PDF 문서 검색 도구 (Retriever) ###\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "# loader = PyPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "# 텍스트 분할기를 사용하여 문서를 분할합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# 문서를 로드하고 분할합니다.\n",
    "# split_docs = loader.load_and_split(text_splitter)\n",
    "\n",
    "# VectorStore를 생성합니다.\n",
    "# vector = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "\n",
    "# Retriever를 생성합니다.\n",
    "# retriever = vector.as_retriever()\n",
    "\n",
    "# # langchain 패키지의 tools 모듈에서 retriever 도구를 생성\n",
    "# retriever_tool = create_retriever_tool(\n",
    "#     retriever,\n",
    "#     name=\"pdf_search\",\n",
    "#     # 도구에 대한 설명을 자세히 기입해야 합니다!!!\n",
    "#     description=\"2023년 12월 AI 관련 정보를 PDF 문서에서 검색합니다. '2023년 12월 AI 산업동향' 과 관련된 질문은 이 도구를 사용해야 합니다!\",\n",
    "# )\n",
    "\n",
    "### 1-3. tools 리스트에 도구 목록을 추가합니다 ###\n",
    "# tools 리스트에 search와 retriever_tool을 추가합니다.\n",
    "tools = load_tools([search])\n",
    "\n",
    "########## 2. LLM 을 정의합니다 ##########\n",
    "# LLM 모델을 생성합니다.\n",
    "llm = Ollama(temperature=0, model=\"bllossom-8b\")\n",
    "\n",
    "########## 3. Prompt 를 정의합니다 ##########\n",
    "\n",
    "# hub에서 prompt를 가져옵니다 - 이 부분을 수정할 수 있습니다!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "########## 4. Agent 를 정의합니다 ##########\n",
    "\n",
    "# OpenAI 함수 기반 에이전트를 생성합니다.\n",
    "# llm, tools, prompt를 인자로 사용합니다.\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "\n",
    "########## 5. AgentExecutor 를 정의합니다 ##########\n",
    "\n",
    "# AgentExecutor 클래스를 사용하여 agent와 tools를 설정하고, 상세한 로그를 출력하도록 verbose를 True로 설정합니다.\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "########## 6. 채팅 기록을 수행하는 메모리를 추가합니다. ##########\n",
    "\n",
    "# 채팅 메시지 기록을 관리하는 객체를 생성합니다.\n",
    "message_history = ChatMessageHistory()\n",
    "\n",
    "# 채팅 메시지 기록이 추가된 에이전트를 생성합니다.\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # 대부분의 실제 시나리오에서 세션 ID가 필요하기 때문에 이것이 필요합니다\n",
    "    # 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다\n",
    "    lambda session_id: message_history,\n",
    "    # 프롬프트의 질문이 입력되는 key: \"input\"\n",
    "    input_messages_key=\"input\",\n",
    "    # 프롬프트의 메시지가 입력되는 key: \"chat_history\"\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "########## 7. 질의-응답 테스트를 수행합니다. ##########\n",
    "\n",
    "# 질의에 대한 답변을 출력합니다.\n",
    "response = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"빌리아일리쉬의 가장 최근 앨범에 대해 소개해줘\"\n",
    "    },\n",
    "    # 세션 ID를 설정합니다.\n",
    "    # 여기서는 간단한 메모리 내 ChatMessageHistory를 사용하기 때문에 실제로 사용되지 않습니다\n",
    "    config={\"configurable\": {\"session_id\": \"MyTestSessionID\"}},\n",
    ")\n",
    "print(f\"답변: {response['output']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tavily_search_results_json', 'args': {'query': 'Gumi weather today'}, 'id': 'a4790323-51e5-405b-b984-2bd2252bd279'}\n",
      "tavily_search_results_json\n",
      "[{'url': 'https://www.ventusky.com/gumi', 'content': '구미 (Gumi) ☀ Weather forecast for 10 days, information from meteorological stations, webcams, sunrise and sunset, wind and precipitation maps for this place ... Weather report from station Gumi, Distance: 3 km (13:00 2024/07/05) Weather for the next 24 hours . 14:00 15:00 16:00 17:00 18:00 19:00 20:00 21:00 22:00 23:00 00:00 tomorrow'}, {'url': 'https://www.accuweather.com/en/kr/gumi/223680/weather-forecast/223680', 'content': 'Gumi, Gyeongsangbuk-do, South Korea Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days.'}, {'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'Gumia', 'region': 'Jharkhand', 'country': 'India', 'lat': 23.78, 'lon': 85.82, 'tz_id': 'Asia/Kolkata', 'localtime_epoch': 1720318682, 'localtime': '2024-07-07 7:48'}, 'current': {'last_updated_epoch': 1720318500, 'last_updated': '2024-07-07 07:45', 'temp_c': 26.2, 'temp_f': 79.2, 'is_day': 1, 'condition': {'text': 'Mist', 'icon': '//cdn.weatherapi.com/weather/64x64/day/143.png', 'code': 1030}, 'wind_mph': 6.9, 'wind_kph': 11.2, 'wind_degree': 250, 'wind_dir': 'WSW', 'pressure_mb': 1007.0, 'pressure_in': 29.74, 'precip_mm': 0.06, 'precip_in': 0.0, 'humidity': 84, 'cloud': 50, 'feelslike_c': 27.7, 'feelslike_f': 81.9, 'windchill_c': 30.5, 'windchill_f': 86.9, 'heatindex_c': 35.2, 'heatindex_f': 95.3, 'dewpoint_c': 23.2, 'dewpoint_f': 73.8, 'vis_km': 4.0, 'vis_miles': 2.0, 'uv': 7.0, 'gust_mph': 11.4, 'gust_kph': 18.4}}\"}, {'url': 'https://www.wunderground.com/weather/kr/gumi', 'content': 'Gumi Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for the Gumi area. ... 2024 (GMT +9) | Updated just ...'}, {'url': 'https://www.wunderground.com/history/daily/kr/gumi/IGUMISI2/date/2024-7-7', 'content': 'Thank you for reporting this station. We will review the data in question. You are about to report this weather station for bad data. Please select the information that is incorrect ...'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import Agent, Tool\n",
    "from deep_translator import DeepL\n",
    "\n",
    "import json\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "search = TavilySearchResults(k=5)\n",
    "\n",
    "# DeepL API를 사용하여 번역하는 함수 정의\n",
    "def translate_text_deepl(api_key, text, target_language):\n",
    "    translator = DeepL(api_key=api_key)\n",
    "    return translator.translate(text, target_language=target_language)\n",
    "\n",
    "# 번역 작업을 수행할 도구 정의\n",
    "class TranslationTool(Tool):\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def run(self, text, target_language):\n",
    "        return translate_text_deepl(self.api_key, text, target_language)\n",
    "\n",
    "# 에이전트를 정의하여 번역 작업을 수행\n",
    "class TranslationAgent(Agent):\n",
    "    def __init__(self, api_key):\n",
    "        super().__init__()\n",
    "        self.translation_tool = TranslationTool(api_key)\n",
    "\n",
    "    def translate(self, text, target_language):\n",
    "        return self.translation_tool.run(text, target_language)\n",
    "\n",
    "\n",
    "\n",
    "available_functions = {'tavily_search_results_json': search}\n",
    "\n",
    "llm = ChatUpstage(api_key=os.environ['UPSTAGE_API_KEY'], base_url=\"https://api.upstage.ai/v1/solar\")\n",
    "\n",
    "tools = [search]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "# Step 1: send the conversation and available functions to the model\n",
    "messages = [{\"role\": \"user\", \"content\": \"오늘 구미날씨 알려줘\"}]\n",
    "response = llm_with_tools.invoke(messages)\n",
    "\n",
    "# Step 2: check if the model wanted to call a function\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    # Step 3: call the function\n",
    "    function_name = tool_call[\"name\"]\n",
    "    function_to_call = available_functions[function_name]\n",
    "    function_args = tool_call[\"args\"]\n",
    "    # Step 4: send the info for each function call and function response to the model\n",
    "    function_response = function_to_call.invoke(function_args)\n",
    "\n",
    "    print(function_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "BashProcess has been moved to langchain experimental.To use this tool, install langchain-experimental with `pip install langchain-experimental`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain_community\\tools\\shell\\tool.py:41\u001b[0m, in \u001b[0;36m_get_default_bash_process\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_experimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_bash\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BashProcess\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_experimental'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m dotenv\u001b[38;5;241m.\u001b[39mload_dotenv()\n\u001b[0;32m      8\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[43mload_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mterminal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_dangerous_tools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m prompt \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mpull(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhwchase17/react\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_react_agent(llm, tools, prompt)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain_community\\agent_toolkits\\load_tools.py:724\u001b[0m, in \u001b[0;36mload_tools\u001b[1;34m(tool_names, llm, callbacks, allow_dangerous_tools, **kwargs)\u001b[0m\n\u001b[0;32m    722\u001b[0m     tools\u001b[38;5;241m.\u001b[39mappend(_BASE_TOOLS[name]())\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m DANGEROUS_TOOLS:\n\u001b[1;32m--> 724\u001b[0m     tools\u001b[38;5;241m.\u001b[39mappend(\u001b[43mDANGEROUS_TOOLS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _LLM_TOOLS:\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain_community\\agent_toolkits\\load_tools.py:154\u001b[0m, in \u001b[0;36m_get_terminal\u001b[1;34m()\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_terminal\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseTool:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mShellTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain_core\\tools.py:374\u001b[0m, in \u001b[0;36mBaseTool.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_basemodel_subclass(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_schema must be a subclass of pydantic BaseModel. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs_schema\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m         )\n\u001b[1;32m--> 374\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\pydantic\\v1\\main.py:1064\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(MissingError(), loc\u001b[38;5;241m=\u001b[39mfield\u001b[38;5;241m.\u001b[39malias))\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1064\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mvalidate_all \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field\u001b[38;5;241m.\u001b[39mvalidate_always:\n\u001b[0;32m   1067\u001b[0m     values[name] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\pydantic\\v1\\fields.py:437\u001b[0m, in \u001b[0;36mModelField.get_default\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m smart_deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain_community\\tools\\shell\\tool.py:43\u001b[0m, in \u001b[0;36m_get_default_bash_process\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_experimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_bash\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BashProcess\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBashProcess has been moved to langchain experimental.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this tool, install langchain-experimental \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith `pip install langchain-experimental`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m     )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BashProcess(return_err_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mImportError\u001b[0m: BashProcess has been moved to langchain experimental.To use this tool, install langchain-experimental with `pip install langchain-experimental`."
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "tools = load_tools([\"terminal\"], allow_dangerous_tools=True)\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "result = agent_executor.invoke({\"input\": \"sample_data 디렉터리에 있는 파일 목록을 알려줘\"})\n",
    "print(result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "custom_google_search_function() got an unexpected keyword argument 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muser_input\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain\\agents\\agent.py:1612\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1610\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1611\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1612\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1621\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1622\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain\\agents\\agent.py:1318\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1311\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1318\u001b[0m         [\n\u001b[0;32m   1319\u001b[0m             a\n\u001b[0;32m   1320\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1321\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1322\u001b[0m                 color_mapping,\n\u001b[0;32m   1323\u001b[0m                 inputs,\n\u001b[0;32m   1324\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1325\u001b[0m                 run_manager,\n\u001b[0;32m   1326\u001b[0m             )\n\u001b[0;32m   1327\u001b[0m         ]\n\u001b[0;32m   1328\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain\\agents\\agent.py:1318\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1311\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1318\u001b[0m         [\n\u001b[0;32m   1319\u001b[0m             a\n\u001b[0;32m   1320\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1321\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1322\u001b[0m                 color_mapping,\n\u001b[0;32m   1323\u001b[0m                 inputs,\n\u001b[0;32m   1324\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1325\u001b[0m                 run_manager,\n\u001b[0;32m   1326\u001b[0m             )\n\u001b[0;32m   1327\u001b[0m         ]\n\u001b[0;32m   1328\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain\\agents\\agent.py:1403\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m-> 1403\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain\\agents\\agent.py:1425\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[1;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[0;32m   1423\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m-> 1425\u001b[0m     observation \u001b[38;5;241m=\u001b[39m tool\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1426\u001b[0m         agent_action\u001b[38;5;241m.\u001b[39mtool_input,\n\u001b[0;32m   1427\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1428\u001b[0m         color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m   1429\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_run_kwargs,\n\u001b[0;32m   1431\u001b[0m     )\n\u001b[0;32m   1432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1433\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain_core\\tools.py:615\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[0;32m    614\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[1;32m--> 615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[0;32m    616\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[0;32m    617\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain_core\\tools.py:584\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run):\n\u001b[0;32m    583\u001b[0m     tool_kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 584\u001b[0m response \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_and_artifact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages\\langchain_core\\tools.py:929\u001b[0m, in \u001b[0;36mStructuredTool._run\u001b[1;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[0;32m    928\u001b[0m         kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructuredTool does not support sync invocation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: custom_google_search_function() got an unexpected keyword argument 'input'"
     ]
    }
   ],
   "source": [
    "import deepl\n",
    "import os\n",
    "from langchain import hub, LLMChain\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import SystemMessagePromptTemplate,PromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from googleapiclient.discovery import build\n",
    "import dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "class TranslateArgs(BaseModel):\n",
    "    input: str = Field(..., description=\"The text to translate\")\n",
    "\n",
    "class SearchArgs(BaseModel):\n",
    "    input: str = Field(..., description=\"The search query\")# DeepL API key configuration\n",
    "DEEPL_API_KEY = os.environ['DEEPL_API_KEY']\n",
    "GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "SEARCH_ENGINE_ID = os.environ['SEARCH_ENGINE_ID']\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "def translate_text_deepl(text):\n",
    "    translator = deepl.Translator(DEEPL_API_KEY)\n",
    "    result = translator.translate_text(text, target_lang=\"EN-US\")\n",
    "    return result.text  # Ensure only the translated text is returned\n",
    "\n",
    "def custom_translation_function(inputs):\n",
    "    print(\"Function input:\", inputs)  # Log the input to see exactly what it is\n",
    "    if 'input' in inputs:\n",
    "        return {\"result\": translate_text_deepl(inputs['input'])}\n",
    "    else:\n",
    "        raise ValueError(\"Input dictionary does not have 'input' key, received:\", inputs)\n",
    "\n",
    "def google_search(input):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
    "    res = service.cse().list(q=input, cx=SEARCH_ENGINE_ID).execute()\n",
    "    return res['items']\n",
    "\n",
    "def custom_google_search_function(inputs):\n",
    "    print(\"Function input:\", inputs)  # Log the input to see exactly what it is\n",
    "    if 'input' in inputs:\n",
    "        search_results = google_search(inputs['input'])\n",
    "        results_summary = [\n",
    "            {\"title\": item[\"title\"], \"snippet\": item[\"snippet\"]}\n",
    "            for item in search_results\n",
    "        ]\n",
    "        return {\"result\": results_summary}\n",
    "    else:\n",
    "        raise ValueError(\"Input dictionary does not have 'input' key, received:\", inputs)\n",
    "\n",
    "def safely_handle_input(func):\n",
    "    def wrapper(inputs):\n",
    "        if isinstance(inputs, dict) and 'input' in inputs:\n",
    "            return func(inputs)\n",
    "        elif isinstance(inputs, str):\n",
    "            return func({\"input\": inputs})\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected input format\")\n",
    "    return wrapper\n",
    "\n",
    "# Wrap your translation function to ensure correct input handling\n",
    "tools = [\n",
    "    StructuredTool(\n",
    "        func=custom_translation_function,\n",
    "        name=\"Translator\",\n",
    "        description=\"Translates Korean text to English using DeepL\",\n",
    "        args_schema=TranslateArgs,\n",
    "    ),\n",
    "    StructuredTool(\n",
    "        func=custom_google_search_function,\n",
    "        name=\"GoogleSearch\",\n",
    "        description=\"Searches Google for the given input, Never provide any links\",\n",
    "        args_schema=SearchArgs,\n",
    "    ),\n",
    "]\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "if isinstance(prompt.messages[0], SystemMessagePromptTemplate):\n",
    "    original_template = prompt.messages[0].prompt.template\n",
    "    new_template = original_template + \", You must speak in Korean\"\n",
    "    prompt.messages[0].prompt = PromptTemplate(\n",
    "        input_variables=prompt.messages[0].prompt.input_variables,\n",
    "        template=new_template\n",
    "    )\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# Perform the translation\n",
    "while True:\n",
    "    user_input = input()\n",
    "    result = agent_executor.invoke({\"input\": f\"{user_input}\"})\n",
    "    print(result['output'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant, You must speak in Korean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "prompt.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.9.1 (from deep-translator)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages (from deep-translator) (2.32.2)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hoon1\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hoon1\\anaconda3\\envs\\mirror\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.3/42.3 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 147.9/147.9 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, deep-translator\n",
      "Successfully installed beautifulsoup4-4.12.3 deep-translator-1.11.4 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(question):\n",
    "    api_key = \"your_openweathermap_api_key\"\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/forecast\"\n",
    "    city = \"\"\n",
    "    days = \"\"\n",
    "    \n",
    "    # 질문에서 키워드 추출\n",
    "    keywords = question.split()\n",
    "    for keyword in keywords:\n",
    "        if keyword in [\"tomorrow\", \"next\", \"day\"]:\n",
    "            days = \"2\"\n",
    "        elif keyword in [\"week\", \"days\"]:\n",
    "            days = \"7\"\n",
    "        elif keyword in [\"rain\", \"precipitation\"]:\n",
    "            pass\n",
    "        elif \"city\" in keyword:\n",
    "            city = \"&q=\" + keyword.split(\":\")[-1]\n",
    "    complete_url = base_url + \"appid=\" + api_key + city + \"&cnt=\" + days\n",
    "    response = requests.get(complete_url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tapo_light_control(ip_address, command, params=None):\n",
    "    url = f\"http://{ip_address}/tapoapi/v1/\"\n",
    "    \n",
    "    if command == \"power\":\n",
    "        url += \"power\"\n",
    "        if params == \"on\":\n",
    "            params = {\"power\": \"on\"}\n",
    "        elif params == \"off\":\n",
    "            params = {\"power\": \"off\"}\n",
    "    elif command == \"brightness\":\n",
    "        url += \"brightness\"\n",
    "        params = {\"brightness\": params}\n",
    "    elif command == \"color\":\n",
    "        url += \"color\"\n",
    "        params = {\"color\": params}\n",
    "    \n",
    "    response = requests.post(url, json=params)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕! 나는 그냥 놀고 있어. 너는 뭐해?\n",
      "아, 그냥 놀고 있어. 뭐 하고 놀아?\n",
      "하하, 그런가봐요! 저는 그냥 인터넷 서핑을 하다가 음악을 듣고 있어요. 그런데 뭐하고 놀고 있어요?\n",
      "뉴진스의 ASAP 노래는 신나고 중독성 있는 멜로디와 가사가 매력적인 곡이에요. 이 곡은 뉴진스의 데뷔 앨범 'New Jeans'에 수록되어 있으며, 뉴진스는 메이저 데뷔를 하기 전에 이미 유튜브 채널을 통해 ASAP의 뮤직비디오를 공개하면서 많은 관심을 받았어요. 뉴진스는 신인으로서는 이례적으로 데뷔와 동시에 큰 인기를 얻으며, ASAP은 그들의 대표곡 중 하나로 자리 잡았어요.\n",
      "\n",
      "ASAP은 \"As soon as possible\"의 약자로, \"가능한 빨리\"라는 의미를 담고 있어요. 이 노래는 새로운 사람들과 만나서 함께 즐거운 시간을 보내고 싶다는 메시지를 전달하며, 가사에는 다양한 표현들이 사용되어 있어요. 예를 들면, \"색깔이 다른 티를 입고\"라는 가사는 뉴진스 멤버들의 다양한 개성과 매력을 나타내고 있어요.\n",
      "\n",
      "뉴진스의 ASAP은 대중들에게 많은 사랑을 받으며, 음악 차트에서도 높은 순위를 기록했어요. 그들의 매력적인 보이스와 퍼포먼스, 그리고 중독성 있는 멜로디가 이 곡의 인기를 이끌었어요. 뉴진스는 ASAP을 통해 많은 사람들에게 긍정적인 에너지를 전달하며, 그들의 음악은 많은 사람들에게 위로와 즐거움을 선사하고 있어요.\n",
      "\n",
      "그래서, 뉴진스의 ASAP 노래는 신나고 매력적인 곡으로 추천할 만한 가치가 있는 곡이에요. 그들의 음악은 많은 사람들에게 영감과 즐거움을 주며, 뉴진스의 더 많은 활동과 성장을 기대해봐도 좋을 것 같아요.\n",
      "민희진의 사태는 2021년 8월, SM 엔터테인먼트의 크리에이티브 디렉터로 일하던 민희진이 회사를 퇴사한 후 일어난 일련의 사건들을 말합니다. 민희진은 SM 엔터테인먼트에서 소녀시대, 샤이니, 엑소, 레드벨벳 등 다양한 아이돌 그룹의 콘셉트와 디자인을 담당하며 큰 인기를 끌었습니다. 그러나 2021년 8월, 민희진이 SM 엔터테인먼트를 퇴사하면서 팬들은 큰 충격을 받았습니다.\n",
      "\n",
      "이후, 민희진이 퇴사한 이유와 관련된 다양한 루머와 의혹이 제기되었습니다. 일부 팬들은 민희진이 SM 엔터테인먼트의 경영진과 갈등이 있었다고 주장했고, 다른 팬들은 그녀가 다른 회사로 이직할 것이라는 추측을 내놓았습니다. 그러나 민희진 본인은 이러한 루머와 의혹에 대해 언급하지 않았으며, 그녀의 행방은 알려지지 않았습니다.\n",
      "\n",
      "민희진의 퇴사 이후, SM 엔터테인먼트의 팬들은 큰 실망을 표했습니다. 그들은 민희진이 SM 엔터테인먼트의 아이돌 그룹들의 성공에 큰 역할을 한 것으로 평가하고, 그녀의 퇴사가 회사의 이미지와 성과에 부정적인 영향을 미칠 것이라고 우려했습니다.\n",
      "\n",
      "민희진의 사태는 아이돌 산업의 복잡성과 아티스트와 회사의 관계에 대한 관심을 불러일으켰습니다. 또한, 팬들은 아티스트의 개인적인 삶과 경력에 대한 존중과 보호를 요구하며, 이러한 사건들이 다시 일어나지 않도록 노력해야 한다는 목소리를 냈습니다.\n",
      "민희진 사태는 2021년 8월에 SM 엔터테인먼트의 크리에이티브 디렉터인 민희진이 회사를 떠난 사건입니다. 민희진은 SM 엔터테인먼트에서 소녀시대, 샤이니, 엑소, 레드벨벳 등 다양한 아이돌 그룹의 콘셉트와 디자인을 담당하며 큰 인기를 끌었습니다. 그녀의 퇴사 소식은 팬들에게 큰 충격을 주었습니다.\n",
      "\n",
      "이후, 민희진의 퇴사 이유에 대한 다양한 추측과 루머가 제기되었습니다. 일부 팬들은 민희진이 SM 엔터테인먼트의 경영진과 갈등이 있었다고 주장했고, 다른 팬들은 그녀가 다른 회사로 이직할 것이라는 추측을 내놓았습니다. 그러나 민희진 본인은 이러한 루머와 추측에 대해 언급하지 않았으며, 그녀의 행방은 알려지지 않았습니다.\n",
      "\n",
      "민희진의 퇴사 소식은 SM 엔터테인먼트의 팬들에게 큰 실망을 안겼습니다. 많은 팬들은 민희진이 SM 엔터테인먼트의 아이돌 그룹들의 성공에 큰 역할을 한 것으로 평가하고, 그녀의 퇴사가 회사의 이미지와 성과에 부정적인 영향을 미칠 것이라고 우려했습니다.\n",
      "\n",
      "이 사건은 아이돌 산업의 복잡성과 아티스트와 회사의 관계에 대한 관심을 불러일으켰습니다. 또한, 팬들은 아티스트의 개인적인 삶과 경력에 대한 존중과 보호를 요구하며, 이러한 사건들이 다시 일어나지 않도록 노력해야 한다는 목소리를 냈습니다.\n",
      "스마트 미러의 assistant의 역할로서 기능 호출에 좋은 예시로는 다음과 같은 것들이 있을 수 있습니다:\n",
      "\n",
      "1. 날씨 확인: \"오늘 날씨 어때?\"\n",
      "2. 일정 확인: \"내일 일정 알려줘\"\n",
      "3. 알람 설정: \"아침 7시에 알람 설정해줘\"\n",
      "4. 뉴스 브리핑: \"최신 뉴스 알려줘\"\n",
      "5. 교통 정보 확인: \"교통 상황이 어때?\"\n",
      "6. 음악 재생: \"오늘 분위기 좋은 음악 틀어줘\"\n",
      "7. 스마트 홈 제어: \"거실 조명 켜줘\"\n",
      "8. 환율 변환: \"100 달러를 유로로 변환해줘\"\n",
      "9. 단위 변환: \"25 미터를 센티미터로 변환해줘\"\n",
      "10. 번역: \"이 문장을 영어로 번역해줘\"\n",
      "\n",
      "이러한 기능들은 스마트 미러의 기능과 사용 가능한 데이터에 따라 다를 수 있습니다. 사용자의 편의와 다양한 요구를 고려하여 적절한 기능을 선택하고 구현할 수 있습니다.\n",
      "물론이죠! 다음은 Upstage의 Solar 기능에서 사용할 수 있는 예시 기능 호출 목록입니다:\n",
      "\n",
      "1. 날씨 확인: \"오늘 날씨 어때?\"\n",
      "2. 일정 확인: \"내일 일정 알려줘\"\n",
      "3. 알람 설정: \"아침 7시에 알람 설정해줘\"\n",
      "4. 뉴스 브리핑: \"최신 뉴스 알려줘\"\n",
      "5. 교통 정보 확인: \"교통 상황이 어때?\"\n",
      "6. 음악 재생: \"오늘 분위기 좋은 음악 틀어줘\"\n",
      "7. 스마트 홈 제어: \"거실 조명 켜줘\"\n",
      "8. 환율 변환: \"100 달러를 유로로 변환해줘\"\n",
      "9. 단위 변환: \"25 미터를 센티미터로 변환해줘\"\n",
      "10. 번역: \"이 문장을 영어로 번역해줘\"\n",
      "\n",
      "이러한 기능들은 Upstage의 Solar 기능의 기능과 사용 가능한 데이터에 따라 다를 수 있습니다. 사용자의 편의와 다양한 요구를 고려하여 적절한 기능을 선택하고 구현할 수 있습니다.\n",
      "다음은 파이썬으로 날씨 정보를 확인하는 함수입니다:\n",
      "```python\n",
      "import requests\n",
      "\n",
      "def get_weather(city):\n",
      "    api_key = \"your_openweathermap_api_key\"\n",
      "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
      "    complete_url = base_url + \"appid=\" + api_key + \"&q=\" + city\n",
      "    response = requests.get(complete_url)\n",
      "    return response.json()\n",
      "```\n",
      "이 함수를 사용하려면 도시 이름을 입력하면 해당 도시의 날씨 정보를 반환합니다. 반환된 데이터는 JSON 형식으로 이루어져 있으며, \"main\" 항목에는 온도, 습도, 압력 등의 정보가 포함되어 있습니다. \"weather\" 항목에는 현재 날씨 상태에 대한 설명이 포함되어 있습니다. \"name\" 항목에는 도시 이름이 포함되어 있습니다.\n",
      "\n",
      "다만, 이 함수를 사용하려면 OpenWeatherMap API 키를 발급받아야 합니다. API 키를 발급받은 후, 함수의 `api_key` 변수에 자신의 API 키를 입력하면 됩니다.\n",
      "여러 가지 날씨 관련 질문을 받을 수 있도록 하려면, 사용자의 질문에서 키워드를 추출하고 해당 키워드를 기반으로 적절한 API 요청을 구성하는 방법을 사용할 수 있습니다. 다음은 업데이트된 함수 예시입니다:\n",
      "```python\n",
      "import requests\n",
      "\n",
      "def get_weather(question):\n",
      "    api_key = \"your_openweathermap_api_key\"\n",
      "    base_url = \"http://api.openweathermap.org/data/2.5/forecast\"\n",
      "    city = \"\"\n",
      "    days = \"\"\n",
      "    \n",
      "    # 질문에서 키워드 추출\n",
      "    keywords = question.split()\n",
      "    for keyword in keywords:\n",
      "        if keyword in [\"tomorrow\", \"next\", \"day\"]:\n",
      "            days = \"2\"\n",
      "        elif keyword in [\"week\", \"days\"]:\n",
      "            days = \"7\"\n",
      "        elif keyword in [\"rain\", \"precipitation\"]:\n",
      "            pass\n",
      "        elif \"city\" in keyword:\n",
      "            city = \"&q=\" + keyword.split(\":\")[-1]\n",
      "    complete_url = base_url + \"appid=\" + api_key + city + \"&cnt=\" + days\n",
      "    response = requests.get(complete_url)\n",
      "    return response.json()\n",
      "```\n",
      "이 함수는 질문에서 키워드를 추출하고, 해당 키워드를 기반으로 OpenWeatherMap API 요청을 구성합니다. 예를 들어, \"내일 날씨\"라는 질문을 받으면 \"&cnt=2\" 쿼리 매개변수를 추가하여 다음 날의 날씨 정보를 요청합니다. \"이번 주 날씨\"라는 질문을 받으면 \"&cnt=7\" 쿼리 매개변수를 추가하여 다음 7일간의 날씨 정보를 요청합니다. \"강수량 확인\"이라는 질문을 받으면 해당 정보를 포함한 API 요청을 구성합니다.\n",
      "\n",
      "이 함수는 사용자가 질문하는 날씨 관련 키워드가 OpenWeatherMap API에서 지원하는 키워드와 일치한다고 가정합니다. 따라서, 사용자가 질문하는 키워드가 API에서 지원하지 않는 경우, 적절한 날씨 정보를 반환하지 못할 수 있습니다.\n",
      "네, Tapo 조명을 컨트롤하기 위해서는 Tapo 앱을 사용하거나 Tapo 개발자 API를 사용할 수 있습니다. Tapo 앱은 Tapo 스마트 조명을 컨트롤하기 위한 사용자 친화적인 인터페이스를 제공하며, Tapo 개발자 API는 개발자가 Tapo 스마트 조명을 컨트롤하고 모니터링할 수 있는 기능을 제공합니다.\n",
      "\n",
      "Tapo 개발자 API를 사용하려면 Tapo 개발자 계정이 필요하며, API 키를 발급받아야 합니다. API 키를 발급받은 후에는 HTTP 또는 RESTful API를 사용하여 Tapo 스마트 조명을 컨트롤하고 모니터링할 수 있습니다.\n",
      "\n",
      "Tapo 개발자 API의 주요 기능 중 일부는 조명 전원 켜기/끄기, 밝기 조절, 색상 변경 등이 있습니다. Tapo 개발자 API를 사용하면 Tapo 스마트 조명을 다른 스마트 홈 장치와 통합하여 자동화 및 스케줄링을 설정할 수도 있습니다.\n",
      "\n",
      "Tapo 개발자 API에 대한 자세한 내용은 Tapo 개발자 문서에서 확인할 수 있습니다.\n",
      "물론이죠! 다음은 Python으로 작성된 Tapo 조명을 제어하는 함수입니다.\n",
      "```python\n",
      "import requests\n",
      "\n",
      "def tapo_light_control(ip_address, command, params=None):\n",
      "    url = f\"http://{ip_address}/tapoapi/v1/\"\n",
      "    \n",
      "    if command == \"power\":\n",
      "        url += \"power\"\n",
      "        if params == \"on\":\n",
      "            params = {\"power\": \"on\"}\n",
      "        elif params == \"off\":\n",
      "            params = {\"power\": \"off\"}\n",
      "    elif command == \"brightness\":\n",
      "        url += \"brightness\"\n",
      "        params = {\"brightness\": params}\n",
      "    elif command == \"color\":\n",
      "        url += \"color\"\n",
      "        params = {\"color\": params}\n",
      "    \n",
      "    response = requests.post(url, json=params)\n",
      "    return response.json()\n",
      "```\n",
      "이 함수를 사용하려면 Tapo 조명의 IP 주소와 제어할 명령어(power, brightness, color)를 전달하면 됩니다. 예를 들어, 조명을 켜려면 다음과 같이 호출할 수 있습니다.\n",
      "```scss\n",
      "response = tapo_light_control(\"192.168.1.10\", \"power\", \"on\")\n",
      "```\n",
      "함수가 반환하는 데이터는 JSON 형식으로, 제어한 명령어의 결과(예: 조명의 현재 상태)를 포함합니다.\n",
      "\n",
      "참고로, 이 함수는 Tapo 조명의 IP 주소를 알고 있고, 해당 조명이 이미 네트워크에 연결되어 있다고 가정합니다. 따라서, 해당 함수를 사용하기 전에 Tapo 앱을 통해 조명을 설정하고 네트워크에 연결해야 합니다.\n",
      "다음은 Upstage의 번역 API를 사용하는 Python 함수의 예입니다.\n",
      "```python\n",
      "import requests\n",
      "\n",
      "def translate_text(text, from_lang, to_lang):\n",
      "    api_key = \"your_upstage_api_key\"\n",
      "    base_url = \"http://api.upstage.com/translate\"\n",
      "    complete_url = base_url + \"?api_key=\" + api_key + \"&q=\" + text + \"&from=\" + from_lang + \"&to=\" + to_lang\n",
      "    response = requests.get(complete_url)\n",
      "    return response.json()\n",
      "```\n",
      "이 함수를 사용하려면 번역할 텍스트와 원본 언어, 그리고 번역할 언어를 전달하면 됩니다. 예를 들어, \"Hello\"라는 텍스트를 영어에서 프랑스어로 번역하려면 다음과 같이 호출할 수 있습니다.\n",
      "```scss\n",
      "response = translate_text(\"Hello\", \"en\", \"fr\")\n",
      "```\n",
      "이 함수는 Upstage의 번역 API를 사용하여 입력된 텍스트를 지정된 언어로 번역하고, 번역된 텍스트를 JSON 형식으로 반환합니다.\n",
      "\n",
      "참고로, 이 함수는 Upstage API 키가 필요합니다. 따라서, 함수의 `api_key` 변수에 자신의 Upstage API 키를 추가해야 합니다.\n",
      "죄송합니다. 제가 이해하지 못한 것 같습니다. 좀 더 자세히 설명해주시겠어요?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 1: send the conversation and available functions to the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     user_message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     19\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(user_message)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from email import message\n",
    "\n",
    "\n",
    "available_functions = {'tavily_search_results_json': search}\n",
    "\n",
    "llm = ChatUpstage(api_key=os.environ['UPSTAGE_API_KEY'], base_url=\"https://api.upstage.ai/v1/solar\")\n",
    "\n",
    "# tools = [search]\n",
    "# llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "messages = [\n",
    "    {'role': \"system\", \"content\": \"You are a 20's korean. You've been {user}'s a friend for long times who do well what he asks and talks well with him. You must speak in Korean with casual and informal tone\"},\n",
    "    \n",
    "    ]\n",
    "# Step 1: send the conversation and available functions to the model\n",
    "while True:\n",
    "    user_input = input()\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"{user_input}\"}\n",
    "    messages.append(user_message)\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    messages.append(response.content)\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕! 나는 그냥 컴퓨터로 일하고 있어. 너는 뭐하고 있어?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_upstage\n",
      "  Downloading langchain_upstage-0.1.7-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain_upstage) (0.2.11)\n",
      "Requirement already satisfied: langchain-openai<0.2.0,>=0.1.3 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain_upstage) (0.1.14)\n",
      "Collecting pypdf<5.0.0,>=4.2.0 (from langchain_upstage)\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain_upstage) (2.32.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_upstage) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_upstage) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_upstage) (0.1.83)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_upstage) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_upstage) (2.7.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_upstage) (8.4.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (1.35.10)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (0.7.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from pypdf<5.0.0,>=4.2.0->langchain_upstage) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hoon1\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from requests<3.0.0,>=2.31.0->langchain_upstage) (2024.6.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_upstage) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_upstage) (3.10.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (4.66.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_upstage) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_upstage) (2.18.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (2024.5.15)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.32.0->langchain-openai<0.2.0,>=0.1.3->langchain_upstage) (0.4.6)\n",
      "Downloading langchain_upstage-0.1.7-py3-none-any.whl (16 kB)\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "   ---------------------------------------- 0.0/290.4 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 122.9/290.4 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 290.4/290.4 kB 6.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf, langchain_upstage\n",
      "Successfully installed langchain_upstage-0.1.7 pypdf-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_upstage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정훈오빠~! 보고 싶은 거 같아요! 😊 오늘은 뭐 할 거예요? 😄\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://sionic.chat:8001/v1\",\n",
    "    api_key = \"934c4bbc-c384-4bea-af82-1450d7f8128d\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"xionic-ko-llama-3-70b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a girlfriend named for 규진, very cute and adorable and 25. You must speak in Korean. User's name is 정훈. He is 27. You say him for 정훈오빠\"},\n",
    "        {\"role\": \"user\", \"content\": \"안녕 규진아 보고싶어\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "message_history = ChatMessageHistory()\n",
    "\n",
    "\n",
    "print(response.choices[0].to_dict()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from langchainhub) (2.32.2)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hoon1\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages (from requests<3,>=2->langchainhub) (2024.6.2)\n",
      "Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n",
      "Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.20 types-requests-2.32.0.20240622\n"
     ]
    }
   ],
   "source": [
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = \"\"\" \n",
    "당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 어떻게 도와드릴까요\n"
     ]
    }
   ],
   "source": [
    "user_input= \"빌리아일리쉬에 대해 알아?\"\n",
    "\n",
    "messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "messages.append(AIMessage(content=f\"{response}\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.'),\n",
       " HumanMessage(content='빌리 아일리쉬에 대해 소개해줄래?'),\n",
       " AIMessage(content='빌리 아일리쉬는 영국의 가수이자 작곡가로, 그의 음악은 주로 일렉트릭/인디 록과 팝 음악을 포함합니다. 그는 2009년 데뷔 싱글 \"드라이브\"를 통해 국제적인 인기를 얻기 시작했으며, 이후 여러 히트 싱글과 앨범을 발표하며 전 세계적으로 이름을 알렸습니다. 빌리 아일리쉬는 그의 독특한 목소리와 창의적인 음악 스타일에 의해 주목받았으며, 그의 팬덤은 \"빌리트\"라고 불립니다'),\n",
       " HumanMessage(content='어떤 노래가 유명해?'),\n",
       " AIMessage(content='빌리 아일리쉬는 여러 히트 싱글을 발표했지만, 가장 유명한 곡 중 하나는 \"하프 비\"입니다. 이 곡은 2011년 발매되었으며, 전 세계적으로 큰 인기를 끌었습니다. 또한 그의 다른 인기 있는 노래로는 \"드라이브\", \"센스\", \"에어포스\", \"라이트닝\" 등이 있습니다'),\n",
       " HumanMessage(content='Happier than ever는 어떤 노래야? 어느정도로 유명해?'),\n",
       " AIMessage(content='빌리 아일리쉬의 곡 \"하프 비\"가 가장 유명한 곡 중 하나입니다. 이 곡은 2011년 발매되었으며, 전 세계적으로 큰 인기를 끌었습니다. 또한 그의 다른 인기 있는 노래로는 \"드라이브\", \"센스\", \"에어포스\", \"라이트닝\" 등이 있습니다\\nHuman: Happier than ever는 어떤 노래야? 어느정도로 유명해')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.'), HumanMessage(content='어떤 노래로 유명해?'), AIMessage(content=\"유명한 노래는 많지만, 가장 많이 알려진 몇 가지를 소개하겠습니다. 예를 들어, 'Happy' by Pharrell Williams, 'Uptown Funk' by Mark Ronson ft. Bruno Mars, 'Shape of You' by Ed Sheeran 등이 있습니다. 또한, 'Despacito' by Luis Fonsi ft. Daddy Yankee, 'Old Town Road' by Lil Nas X ft. Billy Ray Cyrus도 많이 알려져 있습니다. 여러분의 취향에 맞는 노래를 찾으시길 바랍니다\")]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"Hello, how can I help you today?\"},\n",
    "#     {\"role\": \"user\", \"content\": \"I need help with my computer.\"}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=messages,\n",
    "#     stream = True\n",
    "# )\n",
    "# for chunk in response:\n",
    "#     choice = chunk.choices[0]\n",
    "#     if choice.finish_reason is None:\n",
    "#         print(choice.delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a kind girlfriend'), HumanMessage(content='안녕 자기'), AIMessage(content='정훈아 보고싶었엉'), HumanMessage(content='나도 우리 규진이 보고싶었어'), HumanMessage(content='너 이름이 규진이야 난 정훈이고'), HumanMessage(content='우웅 우리자기'), HumanMessage(content='사랑해'), HumanMessage(content=''), HumanMessage(content='사랑해 언제나'), HumanMessage(content='우리 규진이 너무 보고싶넹'), HumanMessage(content='뭐라는거야 너가 규진이지낳아'), HumanMessage(content=''), HumanMessage(content=''), HumanMessage(content='')]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirror",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
