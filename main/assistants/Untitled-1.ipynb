{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import re\n",
    "from flask import Flask, request, jsonify, send_file\n",
    "from flask_cors import CORS\n",
    "import socket\n",
    "import numpy as np\n",
    "import threading\n",
    "import io\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import datetime\n",
    "import os\n",
    "# import main.assistants.modules.LLM as LLM\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def get_whisper(model):\n",
    "    torch_dtype = torch.float16 if model == \"whisper\" else torch.float32\n",
    "\n",
    "    if model == \"whisper\":\n",
    "        model_id = \"openai/whisper-large-v3\"\n",
    "    if model == \"d_whisper\":\n",
    "        model_id = \"distil-whisper/distil-large-v3\"\n",
    "\n",
    "    return model_id, torch_dtype\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model_id, torch_dtype = get_whisper(\"whisper\")\n",
    "\n",
    "whisper = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "whisper.to(device)\n",
    "filtered_phrases = [\"감사합니다.\", \"네.\", \"감사합니다\", \".\", \"네\", \"아멘\"]\n",
    "\n",
    "\n",
    "\n",
    "def do_whisper(audio_buffer, dtype):\n",
    "    audio, sample_rate = sf.read(audio_buffer, dtype='float32')\n",
    "    inputs = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(device, dtype=dtype) \n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"num_beams\": 1,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.4,\n",
    "        \"return_timestamps\": False,\n",
    "        \"language\": \"korean\"\n",
    "    }\n",
    "\n",
    "    # Whisper process\n",
    "    with torch.no_grad():\n",
    "        pred_ids = whisper.generate(input_features, **gen_kwargs)\n",
    "        user_text = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        print(user_text)\n",
    "        print(user_text[0])\n",
    "\n",
    "    if user_text in filtered_phrases:\n",
    "        print(f\"Filtered phrase detected: {user_text}\")\n",
    "        pass\n",
    "\n",
    "    if user_text:\n",
    "        print(f\"Sending user_text: {user_text}\")\n",
    "    else:\n",
    "        user_text = \".\"\n",
    "\n",
    "    return user_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to files/received_audio_0705082608.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:691: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082609.wav\n",
      "[' 안녕하세요']\n",
      " 안녕하세요\n",
      "Sending user_text: [' 안녕하세요']\n",
      "{'timestamp': {'user_text': [' 안녕하세요']}}\n",
      "Audio saved to files/received_audio_0705082609.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082609.wav\n",
      "[' 안녕하세요']\n",
      " 안녕하세요\n",
      "Sending user_text: [' 안녕하세요']\n",
      "{'timestamp': {'user_text': [' 안녕하세요']}}\n",
      "Audio saved to files/received_audio_0705082610.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082610.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082610.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082610.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082610.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082611.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082611.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082611.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082611.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082611.wav\n",
      "[' 안녕하세요']\n",
      " 안녕하세요\n",
      "Sending user_text: [' 안녕하세요']\n",
      "{'timestamp': {'user_text': [' 안녕하세요']}}\n",
      "Audio saved to files/received_audio_0705082611.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082612.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082612.wav\n",
      "[' 안녕하세요.']\n",
      " 안녕하세요.\n",
      "Sending user_text: [' 안녕하세요.']\n",
      "{'timestamp': {'user_text': [' 안녕하세요.']}}\n",
      "Audio saved to files/received_audio_0705082612.wav\n",
      "[' 감사합니다']\n",
      " 감사합니다\n",
      "Sending user_text: [' 감사합니다']\n",
      "{'timestamp': {'user_text': [' 감사합니다']}}\n",
      "Audio saved to files/received_audio_0705082612.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082612.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082612.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082613.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082613.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082613.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082613.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082613.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082614.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082614.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082614.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082614.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082614.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082614.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082615.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082615.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082615.wav\n",
      "[' 안녕하세요']\n",
      " 안녕하세요\n",
      "Sending user_text: [' 안녕하세요']\n",
      "{'timestamp': {'user_text': [' 안녕하세요']}}\n",
      "Audio saved to files/received_audio_0705082615.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082615.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082615.wav\n",
      "[' 안녕']\n",
      " 안녕\n",
      "Sending user_text: [' 안녕']\n",
      "{'timestamp': {'user_text': [' 안녕']}}\n",
      "Audio saved to files/received_audio_0705082616.wav\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Whisper process\u001b[39;00m\n\u001b[0;32m     19\u001b[0m audio_buffer\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Whisper 처리를 위해 다시 스트림의 시작 위치로 이동\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m user_text \u001b[38;5;241m=\u001b[39m \u001b[43mdo_whisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# LLM process\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# gpt_text = LLM.GPT(user_text)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_text}})\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mdo_whisper\u001b[1;34m(audio_buffer, dtype)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Whisper process\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 37\u001b[0m     pred_ids \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mgenerate(input_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[0;32m     38\u001b[0m     user_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(pred_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(user_text)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:578\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[1;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_new_tokens \u001b[38;5;241m+\u001b[39m decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_target_positions:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe length of `decoder_input_ids` equal `prompt_ids` plus special start tokens is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, and the `max_new_tokens` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Thus, the combined length of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mso that their combined length is less than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_target_positions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     )\n\u001b[1;32m--> 578\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    579\u001b[0m     input_features,\n\u001b[0;32m    580\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    581\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m    582\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m    583\u001b[0m     prefix_allowed_tokens_fn\u001b[38;5;241m=\u001b[39mprefix_allowed_tokens_fn,\n\u001b[0;32m    584\u001b[0m     synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m    585\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    587\u001b[0m )\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mreturn_token_timestamps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(generation_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignment_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    590\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_token_timestamps(\n\u001b[0;32m    591\u001b[0m         outputs, generation_config\u001b[38;5;241m.\u001b[39malignment_heads, num_frames\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_frames\n\u001b[0;32m    592\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\transformers\\generation\\utils.py:1597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[0;32m   1592\u001b[0m         inputs_tensor, generation_config\u001b[38;5;241m.\u001b[39mpad_token_id, generation_config\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m   1593\u001b[0m     )\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1597\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\transformers\\generation\\utils.py:523\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[0;32m    521\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    522\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 523\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1209\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[1;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1202\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1203\u001b[0m             hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1206\u001b[0m             output_attentions,\n\u001b[0;32m   1207\u001b[0m         )\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1216\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hoon1\\anaconda3\\envs\\torch-latest\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:783\u001b[0m, in \u001b[0;36mWhisperEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    779\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    780\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m--> 783\u001b[0m     torch\u001b[38;5;241m.\u001b[39misinf(hidden_states)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m    784\u001b[0m ):\n\u001b[0;32m    785\u001b[0m     clamp_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(hidden_states\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmax \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    786\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(hidden_states, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mclamp_value, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mclamp_value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%m%d%H%M%S\")\n",
    "            \n",
    "            # 로컬 오디오 파일 읽기\n",
    "    data, samplerate = sf.read(\"files/spoken_240705042136.wav\")\n",
    "    \n",
    "    # BytesIO 객체에 오디오 데이터 저장\n",
    "    audio_buffer = io.BytesIO()\n",
    "    sf.write(audio_buffer, data, samplerate, format='WAV')\n",
    "    audio_buffer.seek(0)\n",
    "    \n",
    "    # 오디오 데이터를 WAV 파일로 저장 (테스트용)\n",
    "    received_wav_file_path = f\"files/received_audio_{timestamp}.wav\"\n",
    "    with open(received_wav_file_path, 'wb') as f:\n",
    "        f.write(audio_buffer.read())\n",
    "    print(f\"Audio saved to {received_wav_file_path}\")\n",
    "\n",
    "    # Whisper process\n",
    "    audio_buffer.seek(0)  # Whisper 처리를 위해 다시 스트림의 시작 위치로 이동\n",
    "    user_text = do_whisper(audio_buffer, dtype=torch_dtype)\n",
    "\n",
    "    # LLM process\n",
    "    # gpt_text = LLM.GPT(user_text)\n",
    "\n",
    "    print({\"timestamp\": {\"user_text\": user_text}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
